# Copyright (c) Duke University

# Running script requires Graphical Processing Units (GPUs)

# import dependencies
import os, sys
os.makedirs("resources", exist_ok=True)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random
import re
import functools

from sklearn.preprocessing import MaxAbsScaler
from sklearn.model_selection import train_test_split

# dimensionality reduction
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

import argparse
import torch
import torch.nn as nn
import torch.optim as optim
import tqdm
torch.backends.cudnn.benchmark = True


from torch.utils.data import DataLoader
from torch.utils.data import Dataset

# GPU check
device = 'cuda' if torch.cuda.is_available() else 'cpu'
if device == 'cuda':
    print("Run on GPU...")
else:
    print("Run on CPU...")

# change the duration in seconds of p-wave window to run experiments
seconds = 2
#seconds = 3
#seconds = 4

#Hyperparameter Definition
class HyperParams:
    def __init__(self):
        self.INPUT_DIM = 1
        self.BATCH_SIZE = 80
        self.HIDDEN_DIM = 500
        self.OUTPUT_DIM = 1
        self.N_LAYERS = 3
        self.DROPOUT_RATE = 0.5
        self.LR = 0.01
        self.N_EPOCHS = 4
        self.WD = 0
        self.OPTIM = "sgd"
        self.BIDIRECTIONAL = False
        self.SEED = 2

input_dim = 1
hidden_dim=500
layer_dim=3
output_dim=1

#INGEST DATA
#uncomment insert path of data generated by preprocess.py
#path = 

# import data as pandas DataFrame
df = pd.read_csv(path).iloc[:,2:]

# subset data with specified P-wave
sec_idx = seconds*100+1

# extract features and target and convert to torch array
X_num = np.array(df.iloc[:,1:sec_idx], dtype=np.float32)
y_num = np.array(df.iloc[:,0], dtype=np.float32).reshape(X_num.shape[0], 1)


#DATA PRE-PROCESSING
# 1. Split scaled data to training, validation and test
X_train, X_rem, y_train, y_rem = train_test_split(X_num, y_num, train_size=0.8)
X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=1/2)

# 2. Scale the input with MaxAbsScaler
scaler = MaxAbsScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_valid = scaler.transform(X_valid)
X_test = scaler.transform(X_test)

# 3. Create Data Set
class KNET(Dataset):
    def __init__(self, X, y):
        """
        :param X: features (ie. ground accelerations of p-wave)
        :param y: targets (ie. Peak Ground Acceleration)
        """
        self.x = X
        self.y = y

    def __len__(self) -> int:
        return len(self.x)

    def __getitem__(self, idx: int):
        """
        Return the feature and label by the given index.
        :param idx: index of the sample.
        """
        features = self.x[idx]
        target = self.y[idx]

        return features, target

# 4. Instantiate Training, Validation and Test Sets
train_set = KNET(X_train, y_train)
val_set = KNET(X_valid, y_valid)
test_set = KNET(X_test, y_test)

# 5. Creatd Dataloaders for Batch Training
BATCH_SIZE = 10
# train data loader
train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
# validation data loader
val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
# test data loader
test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)

#LSTM
# Weight Initialization
def init_weights(m):
    if isinstance(m, nn.Embedding):
        nn.init.xavier_normal_(m.weight)
    elif isinstance(m, nn.Linear):
        nn.init.xavier_normal_(m.weight)
        nn.init.zeros_(m.bias)
    elif isinstance(m, nn.LSTM) or isinstance(m, nn.GRU):
        for name, param in m.named_parameters():
            if 'bias' in name:
                nn.init.zeros_(param)
            elif 'weight' in name:
                nn.init.orthogonal_(param)

#hyper_parameters that worked
input_dim = 1
hidden_dim=500
layer_dim=3
output_dim=1

class LSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, layer_dim, dropout_rate, output_dim, **kwargs):
        super(LSTM, self).__init__()
        self.hidden_dim = hidden_dim
        # Number of hidden layers
        self.layer_dim = layer_dim
        # Dropout
        self.dropout = dropout_rate
        # LSTM
        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, dropout=self.dropout, batch_first=True) # batch_first=True (batch_dim, seq_dim, feature_dim)
        # Readout layer
        self.fc = nn.Linear(hidden_dim, output_dim)

        # Weight initialization.
        if "weight_init_fn" not in kwargs:
            self.apply(init_weights)
        else:
            self.apply(kwargs["weight_init_fn"])

    def forward(self, x):
        # Initialize hidden state with zeros
        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()
        h0 = h0.to(device)
        # Initialize cell state
        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()
        c0 = c0.to(device)
        # We need to detach as we are doing truncated backpropagation through time (BPTT)
        # If we don't, we'll backprop all the way to the start even after going through another batch
        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))
        out = self.fc(out[:, -1, :]) 
        
        return out
    

#MODEL TRAINING, VALIDATION AND TESTING
# count parameters
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

# train model on train dataloader
def train(dataloader, model, criterion, optimizer, scheduler, device):
    model.train()
    epoch_losses = []

    for batch in tqdm.tqdm(dataloader, desc='training...', file=sys.stdout):
        inputs = batch[0]
        inputs = inputs.view(-1, 200, 1).requires_grad_()
        inputs = inputs.to(device)
        targets = batch[1]
        targets = targets.to(device)
        prediction = model(inputs)
        loss = criterion(prediction, targets)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_losses.append(loss.item())
        scheduler.step()
    return epoch_losses

# evaluate model with validation or test dataloader
def evaluate(dataloader, model, criterion, device):
    model.eval()
    epoch_losses = []

    with torch.no_grad():
        for batch in tqdm.tqdm(dataloader, desc='evaluating...', file=sys.stdout):
            inputs = batch[0]
            inputs = inputs.view(-1, 200, 1).requires_grad_()
            inputs = inputs.to(device)
            targets = batch[1]
            targets = targets.to(device)
            prediction = model(inputs)
            loss = criterion(prediction, targets)
            epoch_losses.append(loss.item())

    return epoch_losses

# learning rate warmup
class ConstantWithWarmup(torch.optim.lr_scheduler._LRScheduler):
    def __init__(
        self,
        optimizer,
        num_warmup_steps: int,
    ):
        self.num_warmup_steps = num_warmup_steps
        super().__init__(optimizer)

    def get_lr(self):
        if self._step_count <= self.num_warmup_steps:
            # warmup
            scale = 1.0 - (self.num_warmup_steps - self._step_count) / self.num_warmup_steps
            lr = [base_lr * scale for base_lr in self.base_lrs]
            self.last_lr = lr
        else:
            lr = self.base_lrs
        return lr
    
def train_and_test_model_with_hparams(hparams, trainloader=train_loader, valloader=val_loader, testloader=test_loader, model_type="lstm", **kwargs):
    # Seeding. DO NOT TOUCH! DO NOT TOUCH hparams.SEED!
    # Set the random seeds.
    torch.manual_seed(hparams.SEED)
    random.seed(hparams.SEED)
    np.random.seed(hparams.SEED)

    # Model
    if "override_models_with_gru" in kwargs and kwargs["override_models_with_gru"]:
        model = GRU(
            hparams.INPUT_DIM, 
            hparams.HIDDEN_DIM,
            hparams.N_LAYERS,
            hparams.DROPOUT_RATE, 
            hparams.OUTPUT_DIM,
            **kwargs)
    else:
        model = LSTM(
            hparams.INPUT_DIM, 
            hparams.HIDDEN_DIM,
            hparams.N_LAYERS,
            hparams.DROPOUT_RATE, 
            hparams.OUTPUT_DIM,
            **kwargs)
    num_params = count_parameters(model)
    print(f'The model has {num_params:,} trainable parameters')

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    # DO NOT change optimizer implementations!
    if hparams.OPTIM == "sgd":
        #optimizer = optim.SGD(model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, momentum=.9)
        optimizer = optim.SGD(model.parameters(), lr=hparams.LR)        
    elif hparams.OPTIM == "adagrad":
        #optimizer = optim.Adagrad(model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6)
        optimizer = optim.Adagrad(model.parameters(), lr=hparams.LR)
    elif hparams.OPTIM == "adam":
        #optimizer = optim.Adam(model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6)
        optimizer = optim.Adam(model.parameters(), lr=hparams.LR)
    elif hparams.OPTIM == "rmsprop":
        #optimizer = optim.RMSprop(model.parameters(), lr=hparams.LR, weight_decay=hparams.WD, eps=1e-6, momentum=.9)
        optimizer = optim.RMSprop(model.parameters(), lr=hparams.LR)
    else:
        raise NotImplementedError("Optimizer not implemented!")

    criterion = nn.MSELoss()
    criterion = criterion.to(device)

    # Start training
    best_valid_loss = float('inf')
    train_losses = 0
    valid_losses = 0
    test_losses = 0
    
    # Path to Save the Model
    model_checkpoint = 'model_small_val_los.pth'

    # Warmup Scheduler. DO NOT TOUCH!
    WARMUP_STEPS = 200
    lr_scheduler = ConstantWithWarmup(optimizer, WARMUP_STEPS)

    for epoch in range(hparams.N_EPOCHS):
        
        # Your code: implement the training process and save the best model.

        # training
        train_losses = train(trainloader, model, criterion, optimizer, lr_scheduler, device)
        
        # evaluation
        valid_losses = evaluate(valloader, model, criterion, device)

        epoch_train_loss = np.mean(train_losses)
        epoch_valid_loss = np.mean(valid_losses)
 
        # Save the model that achieves the smallest validation loss.
        if epoch_valid_loss < best_valid_loss:
            print("Saving ...")
            torch.save(model.state_dict(), model_checkpoint)
            best_valid_loss = epoch_valid_loss

        print(f'epoch: {epoch+1}')
        print(f'train_loss: {epoch_train_loss:.3f}')
        print(f'valid_loss: {epoch_valid_loss:.3f}')


    # Your Code: Load the best model's weights.
    if "override_models_with_gru" in kwargs and kwargs["override_models_with_gru"]:
        model = GRU(
            hparams.INPUT_DIM, 
            hparams.HIDDEN_DIM,
            hparams.N_LAYERS,
            hparams.DROPOUT_RATE, 
            hparams.OUTPUT_DIM,
            **kwargs)
    else:
        model = LSTM(
            hparams.INPUT_DIM, 
            hparams.HIDDEN_DIM,
            hparams.N_LAYERS,
            hparams.DROPOUT_RATE, 
            hparams.OUTPUT_DIM,
            **kwargs)

    # load model learned parameters
    state_dict = torch.load('model_small_val_los.pth')
    model.load_state_dict(state_dict)
    model.cuda()

    # Your Code: evaluate test loss on testing dataset (NOT Validation)
    test_losses = evaluate(testloader, model, criterion, device)

    epoch_test_loss = np.mean(test_losses)

    print(f'test_loss: {epoch_test_loss:.3f}')
    
    # plot training curve
    plt.figure(figsize=(6, 4))
    plt.plot(train_losses, color="orange", label="Train Loss")
    plt.plot(valid_losses, color="magenta", label="Validation Loss")
    #plt.plot(test_losses, color="green", label="Test Loss")
    plt.title("Training Curve for "+str(seconds))
    plt.xlabel("Iterations")
    plt.ylabel("Loss")
    plt.legend()
    plt.show()

    # Free memory for later usage.
    del model
    torch.cuda.empty_cache()
    return {
        'num_params': num_params,
        "test_loss": epoch_test_loss
    }

#HYPERPARAMETER TUNING
# we select adagrad as our optimizer and change the learning rate as recommended
new_hparams = HyperParams()
new_hparams.OPTIM = "adam"

_ = train_and_test_model_with_hparams(new_hparams, train_loader, val_loader, test_loader, "lstm_1layer_base_adagrad_e32_h100")